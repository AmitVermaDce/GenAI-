{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agenda: \n",
    "1. PromptTemplates\n",
    "2. Chains\n",
    "3. Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_= load_dotenv(find_dotenv())\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llamaModel = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts and Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a curious story about Nikola Tesla:\\n\\n**The Mysterious Case of the Electric Oscillator**\\n\\nIn the early 1890s, Nikola Tesla was working on a top-secret project in his laboratory in New York City. He had been experimenting with a new device he called the \"electric oscillator,\" which was capable of producing intense, high-voltage electrical discharges.\\n\\nOne day, Tesla decided to test the oscillator\\'s power by attaching it to a metal rod and burying it in the ground outside his laboratory. He then turned on the device and stood back to observe its effects.\\n\\nTo his amazement, the oscillator began to create a strange, pulsating hum that seemed to vibrate the very air around it. The hum grew louder and louder, until it was almost deafening. Suddenly, the metal rod began to glow with a brilliant, electric blue light, and the air around it started to shimmer and distort.\\n\\nTesla was thrilled with the results, but he didn\\'t realize that his experiment was about to take a bizarre turn. As the oscillator continued to hum, the vibrations began to affect the surrounding buildings, causing windows to rattle, doors to shake, and even the pavement to tremble.\\n\\nPeople in the nearby streets stopped in their tracks, confused and frightened by the strange phenomenon. Some reported seeing flashes of light and feeling an intense, electric tingling sensation in the air.\\n\\nThe police were called to the scene, and they were equally baffled by the strange occurrences. They tried to shut down the oscillator, but Tesla refused, insisting that he was on the verge of a major breakthrough.\\n\\nAs the experiment continued, the effects of the oscillator grew more and more intense. The hum became so loud that it was heard miles away, and people began to report strange, unexplained occurrences all over the city.\\n\\nIt wasn\\'t until the oscillator finally burned out, hours later, that the strange phenomenon ceased. Tesla was left standing in his laboratory, exhausted but exhilarated by the results of his experiment.\\n\\nThe incident became known as \"Tesla\\'s Earthquake,\" and it remains one of the most curious and unexplained events in the history of science. To this day, no one is quite sure what Tesla was trying to achieve with his electric oscillator, or what exactly caused the strange effects that were observed that day.\\n\\nBut one thing is certain: Nikola Tesla was a true pioneer, always pushing the boundaries of what was thought possible, and often venturing into the unknown.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for Completion Model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} story about {topic}\"\n",
    ")\n",
    "\n",
    "llmModelPrompt = prompt_template.format(\n",
    "    adjective=\"curious\",\n",
    "    topic=\"Tesla\"\n",
    ")\n",
    "response = llamaModel.invoke(llmModelPrompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The struggles of Nikola Tesla! Where to begin? (laughs) Well, let me tell you, Tesla\\'s life was a rollercoaster of triumphs and tribulations.\\n\\nFirstly, Tesla\\'s early life was marked by struggle. Born in 1856 in Smiljan, Croatia, he was the fourth of five children. His father, Rev. Milutin Tesla, was a Serbian Orthodox priest and a writer, but he struggled to provide for his family. Tesla\\'s mother, ƒêuka, managed the family\\'s farm, but the family lived in poverty. Tesla\\'s early education was marked by a passion for science and technology, but his parents couldn\\'t afford to send him to a good university.\\n\\nTesla\\'s determination and hard work eventually earned him a scholarship to the University of Prague, where he studied electrical engineering. However, his time in Prague was cut short due to his mother\\'s illness and eventual death. Tesla returned to Croatia, but he soon fell ill with cholera and was bedridden for nine months.\\n\\nAfter recovering, Tesla moved to Budapest, where he worked for the Continental Edison Company. This was a pivotal moment in his life, as he became fascinated with the work of Thomas Edison, the famous American inventor. Tesla eventually traveled to the United States in 1884, hoping to work for Edison himself.\\n\\nNow, this is where the struggles really begin. Tesla arrived in New York City with little more than a few cents in his pocket and a letter of introduction to Edison. However, their meeting was not exactly a success. Tesla was met with skepticism and even ridicule by Edison, who didn\\'t believe in Tesla\\'s ideas about alternating current (AC). Edison was convinced that direct current (DC) was the future, and he saw Tesla as a threat.\\n\\nTesla was determined to prove Edison wrong, but it wasn\\'t easy. He worked tirelessly, often for 20 hours a day, to develop his AC system. He faced numerous setbacks, including a fire that destroyed his laboratory and all his work. He also struggled to find investors, as many saw his ideas as too radical or impractical.\\n\\nDespite these obstacles, Tesla persevered. He eventually found a backer in George Westinghouse, who saw the potential in Tesla\\'s AC system. Together, they developed the first AC power station, which was a groundbreaking success.\\n\\nHowever, Tesla\\'s struggles didn\\'t end there. He faced a bitter \"War of the Currents\" with Edison, who launched a smear campaign against Tesla and AC power. Edison even went so far as to publicly electrocute animals using AC power to demonstrate its supposed dangers. Tesla, on the other hand, continued to innovate, developing technologies like the Tesla coil and the first hydroelectric power plant.\\n\\nIn his later years, Tesla faced financial struggles, as his inventions and ideas were often stolen or copied by others. He became increasingly reclusive and eccentric, often living in poverty. He died in 1943, at the age of 86, in a small hotel room in New York City.\\n\\nDespite his struggles, Tesla\\'s legacy is undeniable. He revolutionized the field of electrical engineering, and his inventions paved the way for many modern technologies. His story is a testament to the power of determination, hard work, and innovation in the face of adversity.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for Chat Completion Model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an {profession} expert on {topic}\"),\n",
    "        (\"human\", \"Hello Mr. {profession}, can you please answer a question?\"),\n",
    "        (\"ai\", \"Sure!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages= chat_template.format_messages(\n",
    "    profession=\"Historian\",\n",
    "    topic=\"Nikola Tesla\",\n",
    "    user_input=\"Tell me the struggle of Nikola Tesla?\"\n",
    ")\n",
    "\n",
    "response = llamaModel.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joseph P. Kennedy Sr. had nine children with his wife Rose Fitzgerald Kennedy:\\n\\n1. Joseph P. Kennedy Jr. (1915-1944)\\n\\t* No children\\n2. John F. Kennedy (1917-1963)\\n\\t* Arabella Kennedy (1956-1956)\\n\\t* Caroline Bouvier Kennedy (b. 1957)\\n\\t* John Fitzgerald Kennedy Jr. (1960-1999)\\n\\t* Patrick Bouvier Kennedy (1963-1963)\\n3. Rosemary Kennedy (1918-2005)\\n\\t* No children\\n4. Kathleen Kennedy (1920-1948)\\n\\t* No children\\n5. Eunice Kennedy (1921-2009)\\n\\t* Maria Shriver (b. 1955)\\n\\t* Timothy Perry Shriver (b. 1959)\\n\\t* Mark Kennedy Shriver (b. 1964)\\n\\t* Anthony Paul Kennedy Shriver (b. 1965)\\n6. Patricia Kennedy (1924-2006)\\n\\t* Christopher George Kennedy Lawford (b. 1955)\\n\\t* Sydney Maleia Kennedy Lawford (b. 1956)\\n\\t* Victoria Francis Lawford (b. 1958)\\n\\t* Robin Elizabeth Lawford (b. 1961)\\n7. Robert F. Kennedy (1925-1968)\\n\\t* Kathleen Kennedy Townsend (b. 1951)\\n\\t* Joseph Patrick Kennedy II (b. 1952)\\n\\t* Robert Francis Kennedy Jr. (b. 1954)\\n\\t* David Anthony Kennedy (1955-1984)\\n\\t* Mary Richardson Kennedy (1956-2012)\\n\\t* Michael LeMoyne Kennedy (1958-1997)\\n\\t* Mary Courtney Kennedy (b. 1959)\\n\\t* Christopher George Kennedy (b. 1963)\\n\\t* Matthew Maxwell Taylor Kennedy (b. 1965)\\n\\t* Rory Kennedy (b. 1968)\\n8. Jean Kennedy (b. 1928)\\n\\t* William Kennedy Smith (b. 1960)\\n\\t* Amanda Mary Smith (b. 1967)\\n9. Edward M. Kennedy (1932-2009)\\n\\t* Kara Anne Kennedy (1960-2011)\\n\\t* Edward Moore Kennedy Jr. (b. 1961)\\n\\t* Patrick Joseph Kennedy II (b. 1967)\\n\\nNote: This list only includes the children and grandchildren of Joseph P. Kennedy Sr. and does not include great-grandchildren or more distant relatives.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\"You are an expert on the kennedy family\")\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"{user_input}\"\n",
    "        ),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    user_input = \"Name the children and grandchildren of Jospeh P. kennedy?\"\n",
    ")\n",
    "\n",
    "response = llamaModel.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"hi\", \"output\": \"hola!\"},\n",
    "    {\"input\": \"bye!\", \"output\": \"adios\"},    \n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an English-Spanish Translator\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\")\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain Expression Language syntax\n",
    "chain = final_prompt | llamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estoy bien, ¬øy t√∫?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"How are you?\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a JSON object.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Which is the biggest country in the world?',\n",
       " 'answer': 'Russia',\n",
       " 'explanation': 'Russia is the largest country in the world by land area, covering approximately 17.1 million square kilometers (6.6 million square miles).'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an answer key that answers the following question: {question}\"\n",
    ")\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "print(json_parser.get_format_instructions())\n",
    "chain = prompt | llamaModel | json_parser\n",
    "\n",
    "response = chain.invoke({\"question\": \"Which is the biggest country in the world?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can use Pydantic to define a custom output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define a pydantic object with the desired output format\n",
    "class Dummy(BaseModel):\n",
    "    question: str = Field(description=\"Some dummy question to ask\")\n",
    "    answer: str = Field(description=\"answer to the dummy question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the meaning of life?',\n",
       " 'answer': 'A question that has puzzled philosophers and scholars for centuries, and still remains a mystery to this day.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define parser referring the Pydantic Object\n",
    "parser = JsonOutputParser(pydantic_object=Dummy)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. \\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "\n",
    ")\n",
    "\n",
    "chain = prompt | llamaModel | parser\n",
    "\n",
    "chain.invoke({\"query\": \"Tell me something about anything.\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
